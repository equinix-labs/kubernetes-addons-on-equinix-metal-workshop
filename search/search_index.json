{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#about-equinix-labs","title":"About Equinix Labs","text":"<p>Equinix Labs offers workshops, proof of concepts, and tools for exploring and bootstrapping Equinix digital infrastructure including Fabric, Metal, and Network Edge.</p>"},{"location":"#about-the-workshop","title":"About the workshop","text":"<p>In this workshop you will learn how to use Terraform to deploy Kubernetes addons with a vanilla Kubernetes cluster on Equinix Metal.</p> <p>The goals of this workshop are:</p> <ul> <li>Become familiar with the Equinix Metal console and tools.</li> <li>Provision Kubernetes addons using Terraform.</li> <li>Configure kubectl to manage Kubernetes.</li> <li>Verify Kubernetes addons on a Kubernetes cluster.</li> </ul>"},{"location":"#workshop-agenda","title":"Workshop agenda","text":"<p>This workshop is split into four parts:</p> Part Title Duration 1 Setup 10 minutes 2 Configuration and Provisioning 10 minutes 3 Addon Deployment Verificatio 8 minutes 4 Conclusion 2 minutes"},{"location":"parts/conclusion/","title":"Conclusion","text":""},{"location":"parts/conclusion/#conclusion","title":"Conclusion","text":"<p>Thank you for participating in the workshop! Let's recap some of the key takeaways that we've learned:</p> <ul> <li>How to customize a Kubernetes deployment using a Terraform module.</li> <li>Deploy a Kubernetes cluster and a Kubernetes add-on with Terraform.</li> <li>How to configure a CLI tool to manage a Kubernetes cluster.</li> <li>How to verify a new controller manager has been deployed in Kubernetes.</li> </ul>"},{"location":"parts/conclusion/#next-steps","title":"Next Steps","text":"<ul> <li>Deploy other add-ons on the Kubernetes cluster deployed in this workshop.</li> <li>Learn how to operate <code>kubectl</code> to troubleshoot deployment errors.</li> </ul>"},{"location":"parts/conclusion/#resources","title":"Resources","text":"<p>Here are a few other resources to look at to continue your Equinix Metal journey:</p> <ul> <li>Deploy @ Equinix: A one-stop shop for blogs, guides, and plenty of other resources.</li> <li>Equinix Metal Docs: Equinix Metal official documentation.</li> <li>Equinix Metal APIs: Programmatically interact with Equinix Metal</li> <li>Equinix Labs: Provides SDKs and Terraform modultes for Infrastructure as Code tools.</li> <li>Equinix Community: A global community for customers and Equinix users.</li> </ul>"},{"location":"parts/part1/","title":"Part 1: Account and API Key Setup","text":"<p>To run this workshop you will need access to an Equinix Metal Account or create a new one following step 1 below.</p> <p>Note:  You are responsible for the cost of resources created in your Equinix Metal account while running this workshop.</p>"},{"location":"parts/part1/#pre-requisites","title":"Pre-requisites","text":"<p>The following tools will be needed on your local development environment where you will be running most of the commands in this guide:</p> <ul> <li>A Unix-like environment (Linux, OSX, Windows WSL)</li> <li>git (v2.0.0+)</li> <li>metal-cli (v0.9.0+)</li> <li>terraform (v1.0.0+)</li> </ul>"},{"location":"parts/part1/#steps","title":"Steps","text":""},{"location":"parts/part1/#1-create-an-equinix-metal-account","title":"1. Create an Equinix Metal account","text":"<p>If you have never used Equinix Metal before, don't worry, you just need 2 minutes to sign-up and create your first organization. If you have any doubt you can watch our Getting Started with Equinix Metal video.</p>"},{"location":"parts/part1/#2-create-an-api-key","title":"2. Create an API key","text":"<p>API keys in Metal can be tied to your user or to a single project. For this workshop we will need a user-level API key.</p> <p>Note: Project API keys do not have access to the entirety of the API; some endpoints can only be used by personal API keys.</p> <p>To create a new user API key, access your user Profile in the Equinix Metal console, click on the User Icon, and click My Profile.</p> <p></p> <ul> <li>Select the <code>API Keys</code> tab.</li> <li>Click on <code>+ Add New Key</code>.</li> <li>Create a new key with <code>Read/Write</code> permissions.</li> </ul> <p></p>"},{"location":"parts/part1/#3-install-and-configure-metal-cli","title":"3. Install and Configure Metal CLI","text":"<p>Once you are familiar with the console you may feel more comfortable managing your Equinix Metal resources with the command-line interface tool Metal-cli.</p> <p>You only need to download a binary to start enjoying Metal-cli, but you may want to check out the different installation options.</p> <p>Once installed you need to configure your credentials. To do this, execute <code>metal init</code> and provide the requested information.</p> <p>Expected output:</p> Bash<pre><code>$ metal init\n\nEquinix Metal API Tokens can be obtained through the portal at https://console.equinix.com/.\nSee https://metal.equinix.com/developers/docs/accounts/users/ for more details.\n\nToken (hidden): \nOrganization ID [e83e4455-e7bf-4a2f-95cf-653b56db1939]: \nProject ID []: \n\nWriting /Users/Username/.config/equinix/metal.yaml\n</code></pre>"},{"location":"parts/part1/#5-install-git-and-terraform","title":"5. Install Git and Terraform","text":"<p>There are many ways to install <code>git</code> and <code>terraform</code> locally. However, we will use a package manager, <code>homebrew</code>, as an example.</p> Bash<pre><code>$ brew install git\n</code></pre> Bash<pre><code>$ brew install terraform\n</code></pre>"},{"location":"parts/part1/#4-verify","title":"4. Verify","text":"Bash<pre><code>$ metal organization get\n\n+--------------------------------------+------------------------------+----------------------+\n|                  ID                  |             NAME             |       CREATED        |\n+--------------------------------------+------------------------------+----------------------+\n| e83e4455-e7bf-4a2f-95cf-653b56db1939 | Equinix Metal Kubernetes POC | 2023-01-01T00:00:00Z |\n+--------------------------------------+------------------------------+----------------------+\n\n$ git -v\ngit version 2.40.0\n\n$ terraform -v\nTerraform v1.5.6\non darwin_arm64\n</code></pre>"},{"location":"parts/part1/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>Can we create API keys without manually using the portal?</li> </ul>"},{"location":"parts/part2/","title":"2. Kubernetes Addons Deployment and Configuration","text":""},{"location":"parts/part2/#part-2-kubernetes-addons-deployment-and-configuration","title":"Part 2: Kubernetes Addons Deployment and Configuration","text":"<p>The following steps require that you be familiar with HashiCorp Terraform modules. For more info, please read Terraform's documentation. The steps below will guide you to deploy a Kubernetes cluster with Kubernetes addons with Terraform modules in an automated fashion.</p>"},{"location":"parts/part2/#steps","title":"Steps","text":""},{"location":"parts/part2/#1-clone-the-terraform-equinix-kubernetes-cluster-repo","title":"1. Clone the terraform-equinix-kubernetes-cluster repo","text":"<p>For this workshop, we will clone the repository <code>terraform-equinix-kubernetes-cluster</code>, to provision a kubernetes cluster. This repository contains a Terraform module that you will run to provision the Kubernetes cluster.</p> Bash<pre><code>$ git clone https://github.com/equinix-labs/terraform-equinix-kubernetes-cluster.git\n$ cd terraform-equinix-kubernetes-cluster\n</code></pre>"},{"location":"parts/part2/#2-customize-the-deployment-with-kubernetes-addons","title":"2. Customize the Deployment with Kubernetes Addons","text":"<p>Now that we have cloned the kubernetes repo, we will need to customize our deployment. Let's first create a subfolder for our custom deployment under the <code>examples</code> folder in the repo and call it <code>my-deployment</code>.</p> Bash<pre><code>$ cd examples &amp;&amp; mkdir my-deployment &amp;&amp; cd my-deployment\n</code></pre> <p>We are going to take a look into the terraform-equinix-kubernetes-addons project and choose which kubernetes addon we would like to deploy into the cluster. By definition, the kubernetes addons project defines a set of Terraform modules. Each module represents a feature or functionality (i.e. load balancing) that users can choose to add into a kubernetes custer deployment. Let's look at those addons closely:</p> Bash<pre><code>$ git clone https://github.com/equinix-labs/terraform-equinix-kubernetes-addons.git\n$ cd terraform-equinix-kubernetes-addons\n$ ls -al modules\n</code></pre> <p>Upon checking the list of addons available, we are going to add the Cloud Provider Equinix Metal (CPEM) addon into our kubernetes deployment.</p> <p>Cloud Controller Manager (CPEM) links the Kubernetes cluster to Metal's APIs. Luckily, there already is a deployment of CPEM under the <code>examples</code> folder. Let's copy the contents under that folder to customize our deployment.</p> Bash<pre><code>$ cp ../cpem-addon/* .\n</code></pre> <p>Now we need to add the addons as a new <code>module</code> in the <code>main.tf</code> file. Let's add the new block as such:</p> Text Only<pre><code>module \"kubernetes_addons\" {\n  source  = \"equinix-labs/kubernetes-addons/equinix\"\n  version = \"0.4.0\"\n\n  ssh_user        = replace(\"root\", module.tfk8s.kubeconfig_ready, \"\")\n  ssh_private_key = var.ssh_private_key_path == \"\" ? join(\"\\n\", [chomp(module.tfk8s.ssh_key_pair[0].private_key_openssh), \"\"]) : chomp(file(var.ssh_private_key_path))\n  ssh_host        = module.tfk8s.kubeapi_vip\n\n  # Wait to run addons until the cluster is ready for them\n  kubeconfig_remote_path = \"/etc/kubernetes/admin.conf\"\n\n  # TODO: These aren't used for CPEM addon but we have to provide them\n  equinix_metro         = var.metal_metro\n  equinix_project       = var.metal_project_id\n  kubeconfig_local_path = \"\"\n\n  enable_cloud_provider_equinix_metal = true\n  enable_metallb                      = false\n\n  cloud_provider_equinix_metal_config = {\n    version = var.cpem_version\n    secret = {\n      projectID    = var.metal_project_id\n      apiKey       = var.metal_auth_token\n      loadbalancer = \"kube-vip://\"\n    }\n  }\n}\n</code></pre> <p>In the block above, We have enabled CPEM with <code>enable_cloud_provider_equinix_metal=true</code> statement:</p> Text Only<pre><code>  enable_cloud_provider_equinix_metal = true\n  enable_metallb                      = false\n</code></pre> <p>We have also set additional variables <code>var.ssh_private_key_path</code> and <code>var.cpem_version</code> required by the CPEM addon to function. Let's define those in the <code>variables.tf</code> file as such:</p> Text Only<pre><code># CPEM variables\nvariable \"ssh_private_key_path\" {\n  description = \"Path of the private key used to SSH into cluster nodes\"\n  sensitive   = true\n  type        = string\n  default     = \"\"\n}\n\nvariable \"cpem_version\" {\n  description = \"Version of the CPEM\"\n  type        = string\n  default     = \"v3.8.0\"\n}\n</code></pre> <p>Now we need to assign the variables used in the <code>module</code> block located in the file <code>main.tf</code> from the previous step with attributes it needs to run. There are multiple ways to accomplish that. For demonstration, we'll populate the <code>terraform.tfvars</code> file with the variable assignments. For more info on <code>terraform.tfvars</code>, see <code>Assign values with a file</code> documentation. For other options, see <code>Customize Terraform configuration with variables</code></p> Bash<pre><code>$ mv terraform.tfvars.example terraform.tfvars\n</code></pre> <p>Assign the <code>metal_auth_token</code> and the <code>metal_project_id</code> variables with the values obtained from Part 1 of the workshop. Keep the <code>metal_metro</code> variable and remove the remaining variable assignments.</p> Terraform<pre><code>metal_auth_token        = \"pUqwuRtmQ3cMZBKodr1arir5GejbFNsp\"\nmetal_project_id        = \"16a060ea-9de3-4cd1-3601-49d38495d426\"\nmetal_metro             = \"da\"\n</code></pre> <p>Note: You may build custom documentation in README.md but it does not necessarily need to be populated in order to provision infrastructure.</p>"},{"location":"parts/part2/#3-provision-kubernetes","title":"3. Provision Kubernetes","text":"<p>Now that we have finished building the terraform plan, we need to apply it. Let's take the same steps demonstrated in <code>Part 3: Apply a Terraform Plan</code> of the the <code>Terraform on Equinix</code> workshop.</p> Bash<pre><code>$ terraform init --upgrade\n$ terraform plan\n$ terraform apply -auto-approve\n</code></pre> <p>Once the terraform apply execution ends, you will see something like this:</p> Terraform<pre><code>Apply complete! Resources: 17 added, 0 changed, 0 destroyed.\n\nOutputs:\n\ntfk8s_outputs = {\n  \"cloud_init_done\" = \"4420275658402557652\"\n  \"kubeconfig_ready\" = \"990381990483175064\"\n  \"kubeip_vip\" = \"145.40.90.140\"\n}\n</code></pre>"},{"location":"parts/part2/#4-install-the-kubernetes-command-line-tool-kubectl","title":"4. Install the Kubernetes Command Line Tool: kubectl","text":"<p>There are many tools available to manage a Kubernetes cluster. For demonstration, we will focus on <code>kubectl</code> as our Kubernetes CLI manager of choice.</p> <p>To install <code>kubectl</code> on MacOS, please follow these instructions.</p> <p>We are ready to run Terraform to provision the kubernetes cluster!</p>"},{"location":"parts/part2/#5-configure-the-kubernetes-command-line-tool-kubectl","title":"5. Configure the Kubernetes Command Line Tool: kubectl","text":"<p><code>kubectl</code> sets the default config file path to <code>$HOME/.kube/config</code>. We will populate this config file with the contents of the file <code>kubeconfig.admin.yaml</code> file generate at the root of the workspace folder <code>my-deployment</code> from the previous Step 3.</p> Bash<pre><code>$ cat kubeconfig.admin.yaml &gt; $HOME/.kube/config\n</code></pre> <p>Alternatively, you could reference the generated config file <code>kubeconfig.admin.yaml</code> directly to manage the cluster. For instance:</p> Bash<pre><code>$ kubectl --kubeconfig kubeconfig.admin.yaml get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   68m\n</code></pre> <p>Now that you have configured <code>kubectl</code>, let's verify its configuration:</p> Bash<pre><code>$ kubectl config view\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://145.40.90.140:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: DATA+OMITTED\n    client-key-data: DATA+OMITTED\n</code></pre> <p>Note that the cluster server IP <code>145.40.90.140</code> matches the <code>kubeip_vip</code> output posted in the outputs of the terraform run completed in Step 3.</p>"},{"location":"parts/part2/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>How many ways can a Kubernetes cluster be managed?</li> </ul>"},{"location":"parts/part3/","title":"3. Addon Deployment Verification","text":""},{"location":"parts/part3/#part-3-verifying-the-deployment-of-kubernetes-addon","title":"Part 3: Verifying the Deployment of Kubernetes addon","text":"<p>We have a Kubernetes cluster and the CPEM addon up and running. Let's verify that the CPEM addon has been successfully deployed into the kubernetes cluster.</p> <p>The steps below will guide you to verify the deplyment of the CPEM addon.</p>"},{"location":"parts/part3/#steps","title":"Steps","text":""},{"location":"parts/part3/#1-identify-the-pod-created-for-cpem","title":"1. Identify the pod created for CPEM","text":"<p>Let's verify that the <code>daemon</code> named <code>cloud-provider-equinix-metal</code> is running on the cluster pod:</p> Bash<pre><code>$ kubectl get daemonset --namespace kube-system cloud-provider-equinix-metal\nNAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ncloud-provider-equinix-metal   1         1         1       1            1           &lt;none&gt;          25m\n</code></pre> <p>Next, we need to find which pod the daemon is running on in the cluster:</p> Bash<pre><code>$ kubectl get pods --namespace kube-system -l app=cloud-provider-equinix-metal\nNAME                                 READY   STATUS    RESTARTS   AGE\ncloud-provider-equinix-metal-zkjqb   1/1     Running   0          30m\n</code></pre>"},{"location":"parts/part3/#2-verify-the-deployment","title":"2. Verify the Deployment","text":"<p>Now that we now which pod the addon is deployed in, let's check the logs of that pod:</p> Bash<pre><code>$ kubectl logs --namespace kube-system cloud-provider-equinix-metal-zkjqb\nI0603 18:33:02.387385       1 serving.go:348] Generated self-signed cert in-memory\nW0603 18:33:02.387421       1 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nI0603 18:33:02.511766       1 config.go:210] authToken: '&lt;masked&gt;'\nI0603 18:33:02.511770       1 config.go:210] projectID: '49d38495-3601-4cd1-9de3-16a060ead426'\nI0603 18:33:02.511772       1 config.go:210] load balancer config: 'kube-vip://'\nI0603 18:33:02.511773       1 config.go:210] metro: ''\nI0603 18:33:02.511774       1 config.go:210] facility: ''\nI0603 18:33:02.511775       1 config.go:210] local ASN: '65000'\nI0603 18:33:02.511776       1 config.go:210] Elastic IP Tag: ''\nI0603 18:33:02.511777       1 config.go:210] API Server Port: '0'\nI0603 18:33:02.511778       1 config.go:210] BGP Node Selector: ''\nI0603 18:33:02.511795       1 config.go:210] Load Balancer ID: ''\nI0603 18:33:02.511808       1 controllermanager.go:152] Version: v3.8.0\nI0603 18:33:02.512617       1 secure_serving.go:213] Serving securely on [::]:10258\nI0603 18:33:02.512669       1 tlsconfig.go:240] \"Starting DynamicServingCertificateController\"\nI0603 18:33:02.512738       1 leaderelection.go:248] attempting to acquire leader lease kube-system/cloud-controller-manager...\nI0603 18:33:02.519099       1 leaderelection.go:258] successfully acquired lease kube-system/cloud-controller-manager\nI0603 18:33:02.519155       1 event.go:294] \"Event occurred\" object=\"kube-system/cloud-controller-manager\" fieldPath=\"\" kind=\"Lease\" apiVersion=\"coordination.k8s.io/v1\" type=\"Normal\" reason=\"LeaderElection\" message=\"k8s-cluster1-pool1-cp-1_2fe04540-8fd1-46cd-bffc-d6f1a53f76e0 became leader\"\nI0603 18:33:02.526317       1 eip_controlplane_reconciliation.go:71] EIP Tag is not configured skipping control plane endpoint management.\nI0603 18:33:02.526333       1 controlplane_load_balancer_manager.go:41] Load balancer ID is not configured, skipping control plane load balancer management\nI0603 18:33:02.735733       1 loadbalancers.go:93] loadbalancer implementation enabled: kube-vip\nI0603 18:33:02.735766       1 cloud.go:104] Initialize of cloud provider complete\nI0603 18:33:02.736310       1 controllermanager.go:311] Started \"service\"\nI0603 18:33:02.736496       1 controller.go:227] Starting service controller\nI0603 18:33:02.736554       1 shared_informer.go:270] Waiting for caches to sync for service\nI0603 18:33:02.736670       1 controllermanager.go:311] Started \"cloud-node\"\nI0603 18:33:02.736741       1 node_controller.go:157] Sending events to api server.\nI0603 18:33:02.736879       1 node_controller.go:166] Waiting for informer caches to sync\nI0603 18:33:02.736978       1 controllermanager.go:311] Started \"cloud-node-lifecycle\"\nI0603 18:33:02.737066       1 node_lifecycle_controller.go:113] Sending events to api server\nI0603 18:33:02.837272       1 shared_informer.go:277] Caches are synced for service\nI0603 18:33:02.837440       1 node_controller.go:415] Initializing node k8s-cluster1-pool1-cp-1 with cloud provider\nI0603 18:33:03.471796       1 node_controller.go:484] Successfully initialized node k8s-cluster1-pool1-cp-1 with cloud provider\nI0603 18:33:03.471957       1 event.go:294] \"Event occurred\" object=\"k8s-cluster1-pool1-cp-1\" fieldPath=\"\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"Synced\" message=\"Node synced successfully\"\nI0603 18:33:04.239497       1 node_controller.go:415] Initializing node k8s-cluster1-pool1-worker-1 with cloud provider\nI0603 18:33:04.862896       1 node_controller.go:484] Successfully initialized node k8s-cluster1-pool1-worker-1 with cloud provider\nI0603 18:33:04.862992       1 event.go:294] \"Event occurred\" object=\"k8s-cluster1-pool1-worker-1\" fieldPath=\"\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"Synced\" message=\"Node synced successfully\"\n</code></pre> <p>Take a look at the logs and note <code>I0603 18:33:02.519099       1 leaderelection.go:258] successfully acquired lease kube-system/cloud-controller-manager</code>. This means that the cloud controller manager operator pod is working as expected and has acquired the lease to run as a controller manager in the cluster.</p>"},{"location":"parts/part3/#3-test-the-add-on","title":"3. Test the Add-on","text":"<p>We have verified that the CPEM addon has been successfully deployed into the kubernetes cluster. Let's test it!</p> <p>We are going to check provider ID associated with the control plane node(s) deployed in the cluster. The provider ID is the unique ID of the instance that an external provider (i.e. cloud provider) can use to identify a specific node. This is a field that is used whenever kubernetes components need to identify one of our nodes to our API.</p> <p>Let's find the control plane node name:</p> Bash<pre><code>$ kubectl get nodes\nNAME                          STATUS     ROLES           AGE    VERSION\nk8s-cluster1-pool1-cp-1       NotReady   control-plane   3d1h   v1.27.5\nk8s-cluster1-pool1-worker-1   NotReady   &lt;none&gt;          3d1h   v1.27.5\n</code></pre> <p>Now let's find the provider ID associated with the <code>k8s-cluster1-pool1-cp-1</code> node:</p> Bash<pre><code>$ kubectl describe nodes k8s-cluster1-pool1-cp-1\nName:               k8s-cluster1-pool1-cp-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m3.small.x86\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=sv\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-cluster1-pool1-cp-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=m3.small.x86\n                    topology.kubernetes.io/region=sv\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 03 Jun 2024 11:32:44 -0700\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node.kubernetes.io/not-ready:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-cluster1-pool1-cp-1\n  AcquireTime:     &lt;unset&gt;\n  RenewTime:       Thu, 06 Jun 2024 15:25:13 -0700\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 06 Jun 2024 15:24:19 -0700   Mon, 03 Jun 2024 11:32:43 -0700   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 06 Jun 2024 15:24:19 -0700   Mon, 03 Jun 2024 11:32:43 -0700   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 06 Jun 2024 15:24:19 -0700   Mon, 03 Jun 2024 11:32:43 -0700   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            False   Thu, 06 Jun 2024 15:24:19 -0700   Mon, 03 Jun 2024 11:32:43 -0700   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\nAddresses:\n  Hostname:    k8s-cluster1-pool1-cp-1\n  ExternalIP:  147.75.71.245\n  InternalIP:  10.67.129.133\nCapacity:\n  cpu:                16\n  ephemeral-storage:  457887756Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             65657932Ki\n  pods:               110\nAllocatable:\n  cpu:                16\n  ephemeral-storage:  421989355231\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             65555532Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 73cf12b09b6340be83c4a2b7a8ebfb67\n  System UUID:                1f145e00-c7d6-11ec-8000-3cecefcde6c2\n  Boot ID:                    fa91db61-0e64-40e7-a298-2cf13ddc83ea\n  Kernel Version:             5.15.0-107-generic\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.32\n  Kubelet Version:            v1.27.5\n  Kube-Proxy Version:         v1.27.5\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   equinixmetal://de9fbfec-c878-4627-be5b-4c368be25ac7\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-provider-equinix-metal-zkjqb                 100m (0%)     0 (0%)      50Mi (0%)        0 (0%)         3d3h\n  kube-system                 etcd-k8s-cluster1-pool1-cp-1                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         3d3h\n  kube-system                 kube-apiserver-k8s-cluster1-pool1-cp-1             250m (1%)     0 (0%)      0 (0%)           0 (0%)         3d3h\n  kube-system                 kube-controller-manager-k8s-cluster1-pool1-cp-1    200m (1%)     0 (0%)      0 (0%)           0 (0%)         3d3h\n  kube-system                 kube-proxy-lkhs9                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d3h\n  kube-system                 kube-scheduler-k8s-cluster1-pool1-cp-1             100m (0%)     0 (0%)      0 (0%)           0 (0%)         3d3h\n  kube-system                 kube-vip-k8s-cluster1-pool1-cp-1                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d3h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (4%)   0 (0%)\n  memory             150Mi (0%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              &lt;none&gt;\n</code></pre> <p>Note the attribute value associated with the <code>ProviderID:</code> key: <code>equinixmetal://de9fbfec-c878-4627-be5b-4c368be25ac7</code>. This value has been issued by CPEM, thus we can conclude that the CPEM addon is working as expected.</p>"},{"location":"parts/part3/#discussion","title":"Discussion","text":"<p>Let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>How is a cloud controller manager deployed in a Kubernetes cluster?</li> <li>How do we verify that the cloud controller manager has been deployed correctly?</li> </ul>"}]}